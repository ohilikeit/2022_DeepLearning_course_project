# 1 주차 - 프로젝트 주제 선정

# 프로젝트 주제 후보

## 1) 한글 텍스트의 Augmentation

  한글 텍스트 데이터를 분류하기위해 모델을 생성하고 학습을 하다보면 많은 어려움을 겪게 된다. 학습 이전에 데이터 자체가 너무 적어 정확도가 잘 나오지 않고 일반화가 되지 않거나 클래스 불균형으로 인해 특정 라벨만 잘 맞추는 이상한 모델이 나오기도 한다. 이를 해결하기 위해 데이터의 수를 증가시키는 Augmentation 기법이 있으나 활발한 연구가 진행되는 영어와 달리 한글은 그에 대한 연구가 많이 없다. 다음은 현재 연구되어 있는 한글 텍스트 증강 기법들이다. 

### [KorEDA(Korean Easy Data Augmentation)](https://github.com/catSirup/KorEDA/tree/master)(김태형)

  이 방법은 [EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks](https://github.com/jasonwei20/eda_nlp) 를 한국어로 쓸 수 있도록 wordnet 부분만 교체한 방법이다.  각 문장에서 위치를 바꾸거나, 새로운 단어를 삽입하거나, 삭제하거나, 대체하여 새로운 문장을 생성하는 방법이다. 

### [ktextaug](https://github.com/jucho2725/ktextaug)(조진욱, 전현규 외 3인)

  한국어에 적용 가능한 변형적 텍스트 증강 기법을 모아둔 패키지로 앞의 EDA 기법에 6가지 노이즈 생성 방법을 추가하고 토크나이징 기능까지 구현되어있다. 

### [데이터 증강 기법을 이용한 한글 개체명 인식](https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE10671972)(조경선, 김성범)

  EDA의 교체, 삽입, 대체, 위치 바꾸기 방법과 태그를 활용한 교체 방법을 보여준다. 

### 기존의 문제점들

  EDA 논문을 보면 영어로 실험해 본 결과 증강된 문장들의 대부분이 원래 문장의 라벨을 보존하고 있다고 한다. 또한 문장들의 길이에 따라 다르지만 증강 이후 성능 향상과 overfitting 방지의 효과도 얻을 수 있다고 한다. 

  하지만 한글의 경우엔 증강 이후 문장의 의미가 달라지는 경우가 생긴다고 한다. 예를 들면, **‘제가 잘못한 건 아닌 것 같아요’** 를 **‘제가 잘못한 총 아닌 것 같아요’ (건 → 총)** 으로 바뀐다. 아 다르고 어 다른 한국어의 특성 상 단순한 기법으로는 문장의 의미를 그대로 유지한 채 증강시키기엔 쉽지 않아 보인다. 

  또한 각 기법들의 코드를 보면 형태소 분석 이전의 문장에서 띄어쓰기를 기준으로 단어들을 구분하여 적용시키기에 띄어쓰기가 되어있지 않은 경우 증강이 전혀 되지 않고 복사만 되는 문제가 생기기도 한다. 

### 해결법?

  아직 문헌 연구나 실험을 해보지 않았기에 정확한 방향은 알 수 없지만 대략적으로 정리해보자면,

- EDA 기법의 경우 형태소 단위의 토크나이징 이후에 적용시키는 방법을 생각해볼 수 있다. 또한 okt의 경우 pos tagging을 통해 태그 교체법을 같이 사용해볼 수 있을 것이다.
- 단어의 교체와 대체의 경우 한글의 특성에 맞는 제약 조건을 거는 방법도 있다. 예를 들어, ‘이적’이라는 단어와 가장 유사한 상위 5개의 단어(입단, 이동, ...)만을 wordnet에서 골라 대체한다. 이때 단어 간 유사도 거리를 측정한 pre-trained vector를 사용할 수 있을 것이다.
- 단어를 자음 + 모음 + 자음(**강 → ㄱ + ㅏ + ㅇ**)의 형태로 쪼개고 각각의 벡터를 미리 지정하고 합치는 형태로 바꾼 후 SMOTE와 같은 데이터 resampling 기법을 적용해볼 수도 있을 것이다.
- 한글 단어의 의미는 붙어있는 부사에 의해 결정된다.(**나는 사과를 먹었다 → ‘는’이 붙은 ‘나’가 주어, ‘를’이 붙은 사과는 목적어)** 따라서 단어 + 부사의 조합 정보를 증강 과정에 활용해볼 수 도 있을 것이다.
- 추가되어도 문장의 의미가 크게 변하지 않는 단어 집합을 만들어 동사, 형용사 앞뒤에 붙인다.(ex) 참, 정말, 대박, 좀, ..) 물론 해당 원소들은 불용어에서 빠져야 하고 적절히 노이즈를 형성하는 수준에서만 이루어져야 할 것이다.

## 2) 신조어를 처리하는 방법론

  정보가 늘어나는 속도는 해가 거듭할수록 비약적으로 늘어나고 그에 따라 새로운 단어, 즉 신조어 역시 기하급수적으로 늘어나고 있다. 이는 인터넷 사용자들이 많이 사용하는, 흔히 밈과 같은 단어 뿐만 아니라 새로운 법률 조항, 사회적 현상, 기업, 은어, 연구 결과, 발견 등과 같이 광범위하다. 따라서 자연어 처리 분야에서 새로운 정보들에 대한 예측력을 높이기 위해선 끊임없이 파생되는 신조어들을 적절히 학습하는 것이 필수적이다. 

### [Unsupervised Neologism Normalization Using Embedding Space Mapping(Nasser Zalmout,? Aasish Pappu† and Kapil Thadani)](http://www.cs.columbia.edu/~kapil/documents/wnut19neologisms.pdf)

  소셜 미디어 콘텐츠에서 신조어를 감지하고 비지도학습을 통해 표준 단어로 정규화하기 위한 접근 방식을 제안하고 있다. 또한 텍스트 정규화 문헌을 기반으로 감지된 신조어의 음성 및 어원적 고려 사항을 포함하여 도입하고 있다. 

### ****[Mining the Web for New Words: Semi-Automatic Neologism Identification with the NeoCrawler(*****Daphné Kerremans and Jelena Prokić)*](https://www.degruyter.com/document/doi/10.1515/ang-2018-0032/html)

  웹에서 새로운 단어를 식별하는 반자동 접근 방식인 NeoCrawler’s Discoverer를 설명한다. 대규모 온라인 텍스트를 개별 토큰으로 처리하고 매우 큰 사전과 대조하는 사전 매칭 절차를 통해 알려지지 않은 문법 시퀀스를 잠재적인 신조어 후보로 자동 식별한다. 이런 후보들은 수동 평가를 위하여 사용자에게 제시되고 지속적인 모니터링을 통해 관리된다. 

### [신조어의 의미 학습을 위한 딥러닝 기반 표적 마스킹 기법(남건민, 서수민, 곽기영)](http://www.koreascience.or.kr/article/CFKO202125036459371.page?&lang=ko)

  저자들은 BERT의 MLM(Masked Language Model)에서 마스크 대상 단어들을 무작위로 선정하지 않고 신조어들에 집중적으로 마스킹을 수행하는 방법을 제안한다. 이를 통해 신조어의 의미 학습이 더욱 정확하게 이루어질 수 있고 실제 실험 결과 역시 우수한 성능을 보임을 확인하였다. 

### 어떻게 하고 싶은가?

  아직 딥러닝 학습 과정에서의 구체적인 방법론은 잘 알지 못하기에 지금 바로 제안할만한 방법론이 떠오르진 않는다. 있는 지식 없는 지식 다 짜내서 아이디어를 내보자면..

- 신조어의 의미를 파악할 때 가장 중요한 점은 문맥이라고 생각한다. 사람들은 새로운 단어를 보면 전후의 단어들과 문맥을 기반으로 어떤 의미일지 예측해보기 때문이다. 문장의 토크나이징 이후 단어 사전과의 사전 비교를 통해 신조어의 비율이 일정 수준 이상일 경우 양방향 모델(bidirectional)모델인 BERT나 GPT-3를 활용한다. 문맥에 맞게 빈칸에 알맞은 단어를 추측하는 작업에 아주 높은 성능을 보이기 때문이다.
- 위의 내용과 표적 마스킹 기법의 연장선으로 신조어에 집중된 마스킹 + 학습된 신조어-기존 단어의 연관 matrix를 가중치로 attention 과정에서 활용하는 방식을 생각해볼 수 있다.
- 신조어를 빠르게 학습하여 모델에 적용하기 위해선 해당 단어의 등장 빈도가 현실세계에서 늘어나는 것을 실시간으로 캐치해야 한다. 기준은 전체가 아닌 최근 1개월 혹은 1~2주 정도로 한정하고 가장 등장가능성이 높은 SNS, 뉴스기사, 커뮤니티를 대상으로 한다.

### 벌써부터 예상되는 한계들..

  사실 최근 공개된 구글의 PaLM과 같은 초거대 인공지능을 활용한다면 신조어들에 대한 처리가 자동으로 이루어질 수도 있다. PaLM은 무손실 어휘집을 만들어 어휘에 없는 유니코드 문자들을 바이트 단위로 분할하여 사용하기 때문이다. 이 부분에 대해선 추가적인 구글링이 필요해보인다. 😂 

  위의 신조어-기존 단어 연관 matrix 역시 아예 새로운 의미가 탄생한 신조어의 경우 신뢰성이 떨어진다거나 전체 단어 사전에 대한 matrix(그것도 계속해서 크기가 증가하는) 연산이 매 epoch마다 이루어진다면 가용 자원의 관점에서도 더 나은 방법을 찾아봐야 할 것이다.

  특히나 한국어의 경우 신조어의 의미를 파악하기가 다른 언어에 비해 훨씬 힘들다. 예를 들면, 야민정음(**멍멍이 → 댕댕이**), 다른 나라의 언어로 한글처럼 보이게 하는 경우(**인도 구자라트어, 더러워 →લસશ**), 법률 이름에 붙은 이름(**민식이 법**) 등이 있다. 한국인은 이해할 수 있지만 기계의 관점에선 해당 문맥에서 전혀 상관없는 단어가 의미가 있다고 학습해야하는 일이 생긴다. 이를 해결하기 위해 수많은 parameter들을 가진 모델을 사용한다면 처음에 언급한 한계로 돌아가게 된다. 😑