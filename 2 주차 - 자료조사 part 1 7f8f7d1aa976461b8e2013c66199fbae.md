# 2 주차 - 자료조사 part.1

# 1.  최종 주제 선정

## 멀티모달 딥러닝 서비스에 대한 적대적 공격 방어법

  최종적으로 선택된 프로젝트의 주제는 ‘멀티모달 딥러닝 서비스에 대한 적대적 공격 방어법’ 이다. **멀티모달**이란 ‘멀티모달 인터페이스(Multimodal Interface)’ 를 줄인 말로 사람과 기계 간 통신을 위해 음성, 키보드, 펜 등을 이용해 정보를 주고받는 것을 말한다. 다양한 자원으로부터 수집된 데이터로 학습된 딥러닝 모델을 기반으로한 서비스에서 적대적 공격에 대해 어떻게 방어할 것인가를 알아보고자 한다.

![Untitled 1](https://user-images.githubusercontent.com/37128004/169528815-f1d3c5f1-af72-4632-a622-a397f41a19dc.png)

  **적대적 예제(adversarial example)**는 공격하고자 하는 신경망이 정확히 분류해야하는 샘플에 인간의 시각 시스템에는 탐지되지 않는 적은 양의 잡음을 추가하여 해당 신경망이 잘못 인식하게 되는 샘플을 의미하고 이러한 예제를 이용하여 딥러닝 시스템을 무력화시키는 공격을 **적대적 공격(adversarial attack)**이라고 정의한다. 이런 적대적 공격 방법에는 **FGSM(Fast GradientSign Method), BIM(Basic Iterative Method)** 그리고 **CW(Carlini and Wagner)** 등이 대표적이다. 

![Untitled 2](https://user-images.githubusercontent.com/37128004/169528843-9d721e5c-c8ad-43f7-b7ca-cf0a3788d91b.png)

  **적대적 방어 기법**에는 크게 **적대적 학습(adversarial training)** 기법과 **전처리(preprocessing)**기법이 있다. 적대적 학습엔 **FGSM**을 이용한 학습법과 그 연장선인 **PGD(Projected Gradient Descent)**기법이 사용되며 전처리 기법으론 **이진 검출기 사용법, MagNet, Defense-GAN** 방법이 사용된다. 

![Untitled 3](https://user-images.githubusercontent.com/37128004/169528868-5759336b-279a-4d20-9246-4047f81115b6.png)

# 2. 이미지-텍스트 기반 멀티모달 서비스

## 네이버 스마트렌즈 - 멀티모달 AI ‘옴니서치’ 적용

  **네이버**가 자사의 딥러닝 기반 이미지 검색 서비스 ‘**스마트렌즈**’에 멀티모달 AI 모델을 적용하였다. ‘**스마트 렌즈’**는 사용자가 검색하고 싶은 이미지를 촬영하면 다양한 주제에 특화된 검색을 해주는 서비스로 **‘+검색어 추가’** 기능을 통해 텍스트를 추가하여 더욱 구체화된 정보를 찾을 수 있게 된다. 현재는 ‘**스니커즈(sneakers)**’ 카테고리에 적용되었으며 추후 다양한 카테고리로 확대할 계획이라고 한다. 

![Untitled 4](https://user-images.githubusercontent.com/37128004/169528904-c1b1a5bc-4111-4b0a-b90b-cd448efdf971.png)

## 카카오브레인 - 20억 건의 텍스트-이미지 데이터셋 구축

  **카카오브레인**은 최근 명령어를 입력하면 맥락에 맞게 그림을 그려내는 초거대 AI 멀티모달 ‘**민달리(minDALL-E)**’를 선보였다. 이보다 더 높은 수준의 AI 멀티모달을 위하여 20억 건의 텍스트-이미지 데이터셋을 구축하였고 일부를 공개할 예정이라고 한다. 

![Untitled 5](https://user-images.githubusercontent.com/37128004/169528937-37ab8bd5-13e3-4828-9409-0f6db243d792.png)

## OpenAI - Text2Art

OpenAI가 개발한 **CLIP**은 이미지와 텍스트를 동시에 고려하는 멀티모달 모델이다. 이를 활용한 **Text2Art**라는 사이트가 생겼다. 텍스트로 설명하면 그림을 그려주는 서비스인데 텍스트와 추가로 keywords, quailty, type, aspect ratio, initial image 를 상세하게 지정해주면 더 자세한 그림을 그려준다.

  Text2Art는 DALLE와 다르게 VQGAN을 사용한다. VQGAN이 이미지를 만들면 CLIP이 해당 텍스트와 일치하는지 검사하는 과정을 여러번 반복하여 텍스트 설명과 유사한 이미지가 생성된다. 사람이 텍스트를 보고 속으로 떠올리는 이미지가 형성되는 과정과 비슷하게 딥러닝으로 구현했다. 

![Untitled 6](https://user-images.githubusercontent.com/37128004/169528974-18b8db3f-7639-42db-878d-67d3d247efe3.png)

# 3. 적대적 공격을 성공적으로 방어한 언어 모델

## Defending Multimodal Fusion Models against Single-Source Adversaries

  위 [논문](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Defending_Multimodal_Fusion_Models_Against_Single-Source_Adversaries_CVPR_2021_paper.pdf)에선 표준적인 Multimodal 융합 모델이 **single-source adversaries**에 취약함을 보여주며 모든 입력 소스로부터 오는 정보를 비교하도록 모델을 훈련시키는 적대적으로 강력한 융합 전략을 제안한다. 연구 결과, 교란되지 않은 데이터의 성능은 저하시키지 않으며 single-source 견고성의 최첨단 방법을 크게 개선하였다. 행동인지엔 7.8%-25.2%, 객체 탐지엔 19,7%-48.2%, 감정 분석엔 1.6%-6.7%의 성능향상을 보였다. 

  **odd one-out learning**을 통해 멀티모달 consistency를 활용하는 **강력한 feature fusion**을 기반으로 효과적인 솔루션을 제공한다고 한다. 여기서 **odd-one out**이란 **‘한 그룹에서 남들과 다른 것’**을 뜻하는 관용표현으로 여러 요소로 표현되는 보조적인 자체 감독 task에서 어떤 요소가 다른 요소와 다른지 예측하는 방법이다. 쉽게 말해, 교란된 모달리티에서 탐지된 불일치 정보를 다른 모달리티와 비교하고 교란되지 않은 모달리티의 정보만을 차용하는 방식으로 보면 된다. 

![Untitled 7](https://user-images.githubusercontent.com/37128004/169529011-16575493-29a4-4f1b-b7b5-2cdc9476cb00.png)

## FireBERT : Hardening BERT Classifiers Against Adversarial Attack

  위 [논문](https://arxiv.org/abs/2008.04203)에선 **TextFooler-style**의 적대적 공격에 대응하여 3 가지 proof-of-concept NLP classifiers를 강화하는 **FireBERT**를 소개한다. 여기서 **TextFooler**란 자연어처리 시스템을 무력화시키는 프로그램으로 문장에서 주요 단어를 동의어로 대체하여 사람은 동일의미로 받아들이지만 시스템은 전혀 다른 의미로 받아들여 오류율을 높인다. 

  첫 번째로 훈련 데이터와 합성된 적대 샘플에 대해 BERT를 **공동 조정(co-tune)**한다. 두 번째로 단어의 대체와 임베딩 벡터의 섭동을 통해 평가 시에 합성 샘플들을 만들어낸다. 이후 다양하게 생성된 평가 결과들을 투표로 합산한다. 세 번째로 평가 시에 임베딩 벡터의 섭동으로 대체한다. 

![Untitled 8](https://user-images.githubusercontent.com/37128004/169529057-939321fa-b134-4006-b4c6-4611821f75e4.png)

  FireBERT로 BERT 모델을 강화한 결과, 적대적 공격을 받은 후에도 이전의 모델 성능에서 크게 줄어들지 않은 정확도를 얻을 수 있었다. 저자들은 95%의 미리 생성된 적대적 예제들로부터 원래 모델 performance의 98%를 유지하며 모델을 보호할 수 있는 **합성 데이터 생성기와의 공동 조정(co-tuning with a synthetic data generator)**를 제안한다. 또한 98%의 performance를 유지하며 75% 이상의 정확도를 복구할 수 있는 **평가시간 섭동(evaluation-time perturbation)**을 제안하고 있다. 결론적으로, BERT-based classifier들이 미리 만들어진 적대적 예제들과 TextFooler 매커니즘에 의한 공격에 대해 강화될 수 있음을 시사한다. 

## DARCY - 허니팟 기반의 자연어 적대적 공격 방어법

   **DARCY** 모델은 **허니팟(honeypot)**이라는 사이버 보안 개념을 사용하여 가짜 뉴스 탐지기 및 스팸 필터와 같은 자연어 처리 애플리케이션에 대한 잠재적인 공격을 미끼를 통해 잡아낸다. 허니팟이란 비정상 접근을 탐지하기 위하여 의도적으로 설치해 둔 시스템을 의미한다. DARCY는 텍스트 신경망에 여러 트랩도어 또는 미끼를 검색하고 주입하여 범용 트리거 공격에 의해 생성된 악성 콘텐츠를 포착하고 필터링한다. 

![Untitled](https://user-images.githubusercontent.com/37128004/169529086-4a01ca26-bb82-4e14-b9cd-865747aa70b6.png)

  현재의 적대적 공격 방어 방법은 대체로 반응적이기에 방어자가 공격 후 해커의 기술을 관찰하고 학습한 후 다른 공격이 탐지되고 제거되는 것을 기다려야 한다. 하지만 DARCY는 사전적 예방방식으로 공격을 방지하는데 도움이 되고 실험 결과 기존의 여러가지 적대적 탐지 알고리즘을 능가했다고 한다. 

# References

[연구개발특구진흥재단](https://www.innopolis.or.kr/board/view?pageNum=9&rowCnt=10&no1=869&linkId=46226&menuId=MENU00999&schType=0&schText=&boardStyle=&categoryId=&continent=&country=)

[[PyTorch] 적대적 공격(Adversarial Attack) - FGSM/PGD](https://rain-bow.tistory.com/entry/%EC%A0%81%EB%8C%80%EC%A0%81-%EA%B3%B5%EA%B2%A9Adversarial-Attack-FGSMPGD)

[Text2Art](https://text2art.com/)

[네이버, '스마트렌즈'에 멀티모달 AI '옴니서치' 적용](http://www.itdaily.kr/news/articleView.html?idxno=207811)

[[Pytorch-기초강의] 7. 딥러닝을 해킹하는 적대적 공격](https://yjs-program.tistory.com/171)

[Honeypot security technique can also stop attacks in natural language processing | Penn State University](https://www.psu.edu/news/research/story/honeypot-security-technique-can-also-stop-attacks-natural-language-processing/)
